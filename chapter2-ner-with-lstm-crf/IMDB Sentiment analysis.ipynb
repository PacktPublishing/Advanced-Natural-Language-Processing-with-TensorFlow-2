{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "# check if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of GPU config section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract_reasoning, aeslc, aflw2k3d, amazon_us_reviews, arc, bair_robot_pushing_small, beans, big_patent, bigearthnet, billsum, binarized_mnist, binary_alpha_digits, c4, caltech101, caltech_birds2010, caltech_birds2011, cars196, cassava, cats_vs_dogs, celeb_a, celeb_a_hq, cfq, chexpert, cifar10, cifar100, cifar10_1, cifar10_corrupted, citrus_leaves, cityscapes, civil_comments, clevr, cmaterdb, cnn_dailymail, coco, coil100, colorectal_histology, colorectal_histology_large, cos_e, curated_breast_imaging_ddsm, cycle_gan, deep_weeds, definite_pronoun_resolution, diabetic_retinopathy_detection, div2k, dmlab, downsampled_imagenet, dsprites, dtd, duke_ultrasound, dummy_dataset_shared_generator, dummy_mnist, emnist, eraser_multi_rc, esnli, eurosat, fashion_mnist, flic, flores, food101, gap, gigaword, glue, groove, higgs, horses_or_humans, i_naturalist2017, image_label_folder, imagenet2012, imagenet2012_corrupted, imagenet_resized, imagenette, imagewang, imdb_reviews, iris, kitti, kmnist, lfw, librispeech, librispeech_lm, libritts, lm1b, lost_and_found, lsun, malaria, math_dataset, mnist, mnist_corrupted, movie_rationales, moving_mnist, multi_news, multi_nli, multi_nli_mismatch, natural_questions, newsroom, nsynth, omniglot, open_images_v4, opinosis, oxford_flowers102, oxford_iiit_pet, para_crawl, patch_camelyon, pet_finder, places365_small, plant_leaves, plant_village, plantae_k, qa4mre, quickdraw_bitmap, reddit_tifu, resisc45, rock_paper_scissors, rock_you, scan, scene_parse150, scicite, scientific_papers, shapes3d, smallnorb, snli, so2sat, speech_commands, squad, stanford_dogs, stanford_online_products, starcraft_video, sun397, super_glue, svhn_cropped, ted_hrlr_translate, ted_multi_translate, tf_flowers, the300w_lp, tiny_shakespeare, titanic, trivia_qa, uc_merced, ucf101, vgg_face2, visual_domain_decathlon, voc, wider_face, wikihow, wikipedia, wmt14_translate, wmt15_translate, wmt16_translate, wmt17_translate, wmt18_translate, wmt19_translate, wmt_t2t_translate, wmt_translate, xnli, xsum, yelp_polarity_reviews'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available tfds data sets\n",
    "\", \".join(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TFDS dataset\n",
    "# note: as_supervised converts dicts to tuples\n",
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", \n",
    "                                with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", \n",
    "                      as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    version=1.0.0,\n",
      "    description='Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=100000,\n",
      "    splits={\n",
      "        'test': 25000,\n",
      "        'train': 25000,\n",
      "        'unsupervised': 50000,\n",
      "    },\n",
      "    supervised_keys=('text', 'label'),\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
      " tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for example, label in imdb_train.take(1):\n",
    "    print(example, '\\n', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default tokenizer settings\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "MAX_TOKENS = 0\n",
    "\n",
    "for example, label in imdb_train:\n",
    "  some_tokens = tokenizer.tokenize(example.numpy())\n",
    "  if MAX_TOKENS < len(some_tokens):\n",
    "        MAX_TOKENS = len(some_tokens)\n",
    "  vocabulary_set.update(some_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93931 2525\n"
     ]
    }
   ],
   "source": [
    "imdb_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set, \n",
    "                                                   tokenizer=tokenizer)\n",
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "print(vocab_size, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "# Lets verify tokenization and encoding works\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example)\n",
    "    encoded = imdb_encoder.encode(example.numpy())\n",
    "    print(imdb_encoder.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation fucntions to be used with the dataset\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "def encode_pad_transform(sample):\n",
    "    encoded = imdb_encoder.encode(sample.numpy())\n",
    "    pad = sequence.pad_sequences([encoded], padding='post', \n",
    "                                 maxlen=150)\n",
    "    return np.array(pad[0], dtype=np.int64)  \n",
    "\n",
    "\n",
    "def encode_tf_fn(sample, label):\n",
    "    encoded = tf.py_function(encode_pad_transform, \n",
    "                                       inp=[sample], \n",
    "                                       Tout=(tf.int64))\n",
    "    encoded.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the transformation on a small subset\n",
    "subset = imdb_train.take(10)\n",
    "tst = subset.map(encode_tf_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[17742 56635 49898  9860 43863 69648 88571  2913 74960 65318 59132 43610\n",
      " 54077 90053 62861 53649  1572 49300 92378 40939 10306 14306 14152 13803\n",
      " 17115 74960 51007 20831 81120 59132 86909 46281 51007 40939 26463 81363\n",
      " 75536 76259 14152 69648  8531 12165  2659 17742 69648 30986 49898 28902\n",
      " 33424 16784 89115 23845 20418 88861 78870 92944 65227 33128 17915 64804\n",
      " 13263 20702 65227  8126 51007 88793 40982 11237 53775 41392 22350 39116\n",
      "  1873 34111 32416  8552 34986 74425 51911 90053 56635 78267 14306 59822\n",
      " 78870 17297 73762 59132 59822 69648 75561 56635 37594 47467 34379  4485\n",
      " 36749 81188 77632 38691 75561 76586 92378 74083 70330 14152 27519 60108\n",
      "  8531 70330 54077 90053  8531 10595 24278 81188 81363 22821 82725 93731\n",
      " 80234     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "for review, label in tst.take(1):\n",
    "    print(review, label)\n",
    "    print(imdb_encoder.decode(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now tokenize/encode/pad all training\n",
    "# and testing data\n",
    "encoded_train = imdb_train.map(encode_tf_fn,\n",
    "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "encoded_test = imdb_test.map(encode_tf_fn,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (100, None, 64)           6011584   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (100, 64)                 33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 1)                  65        \n",
      "=================================================================\n",
      "Total params: 6,044,673\n",
      "Trainable params: 6,044,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_lstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetch for performance\n",
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.4175 - accuracy: 0.8011 - Precision: 0.7806 - Recall: 0.8378\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.1829 - accuracy: 0.9344 - Precision: 0.9346 - Recall: 0.9342\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.1060 - accuracy: 0.9650 - Precision: 0.9659 - Recall: 0.9641\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0746 - accuracy: 0.9756 - Precision: 0.9764 - Recall: 0.9748\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0631 - accuracy: 0.9796 - Precision: 0.9815 - Recall: 0.9776\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0450 - accuracy: 0.9860 - Precision: 0.9852 - Recall: 0.9868\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0441 - accuracy: 0.9857 - Precision: 0.9859 - Recall: 0.9854\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0353 - accuracy: 0.9886 - Precision: 0.9899 - Recall: 0.9874\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0365 - accuracy: 0.9886 - Precision: 0.9897 - Recall: 0.9876\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.0216 - accuracy: 0.9932 - Precision: 0.9936 - Recall: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd8e845f1d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    250/Unknown - 20s 79ms/step - loss: 0.7937 - accuracy: 0.8232 - Precision: 0.8852 - Recall: 0.7428"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7936793555021286, 0.82324, 0.8852131, 0.7428]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.5\n",
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, dropout=0.5)),\n",
    "    #tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    #tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (50, None, 128)           12023168  \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (50, None, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (50, 128)                 98816     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (50, 1)                   129       \n",
      "=================================================================\n",
      "Total params: 12,220,929\n",
      "Trainable params: 12,220,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm = build_model_bilstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.3734 - accuracy: 0.8278 - Precision: 0.8213 - Recall: 0.8378\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.1497 - accuracy: 0.9443 - Precision: 0.9444 - Recall: 0.9442\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.0814 - accuracy: 0.9718 - Precision: 0.9724 - Recall: 0.9711\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 64s 127ms/step - loss: 0.0510 - accuracy: 0.9822 - Precision: 0.9824 - Recall: 0.9821\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.0263 - accuracy: 0.9904 - Precision: 0.9909 - Recall: 0.9900\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 91s 182ms/step - loss: 0.0226 - accuracy: 0.9921 - Precision: 0.9921 - Recall: 0.9922\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 103s 205ms/step - loss: 0.0192 - accuracy: 0.9934 - Precision: 0.9934 - Recall: 0.9934\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 63s 127ms/step - loss: 0.0186 - accuracy: 0.9940 - Precision: 0.9935 - Recall: 0.9945\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 64s 128ms/step - loss: 0.0161 - accuracy: 0.9946 - Precision: 0.9948 - Recall: 0.9943\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 63s 125ms/step - loss: 0.0122 - accuracy: 0.9957 - Precision: 0.9956 - Recall: 0.9958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd88d854950>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    500/Unknown - 20s 39ms/step - loss: 1.0473 - accuracy: 0.8355 - Precision: 0.8025 - Recall: 0.8899"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0472896423190832, 0.83548, 0.8025395, 0.88992]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2gpu",
   "language": "python",
   "name": "tf2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
